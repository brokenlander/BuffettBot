{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9010653-f6a6-47af-a526-8cd5ab610cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfc40ad-8416-4857-ab12-8b3c53ac9017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 2. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747180e47f534e7fae9eca66cdfb3ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    \"unsloth/Mistral-Small-3.1-24B-Base-2503\",\n",
    "    \"unsloth/Qwen3-32B\",\n",
    "    \"unsloth/gemma-3-27b-it\",\n",
    "    \"unsloth/Llama-3.3-70B-Instruct\"\n",
    "] \n",
    "\n",
    "dtype = None\n",
    "load_in_4bit = False\n",
    "max_seq_length = 8192\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = models[3],\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    device_map = \"auto\",\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd2324e-993e-4934-86df-270cd4807f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Applying QAT to mitigate quantization degradation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.3 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    "    \n",
    "    qat_scheme = \"fp8-fp8\",  # Options: \"int4\", \"fp8-int4\", \"fp8-fp8\", \"int8-int4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daec0f4c-91ba-49ec-a54a-9794aed7ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.3\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5659e731-e962-4d65-843e-0b400ce47c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files = \"/home/ubuntu/BuffettBot/Dataset/Processed/Ground Truth/dataset_combined.json\",\n",
    "    split = \"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2a36b4-a3a4-4704-893e-38a0acdae728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'human',\n",
       "  'value': 'How do you approach the challenge of maintaining discipline when market conditions soften and competitors start offering unrealistic rates?'},\n",
       " {'from': 'gpt',\n",
       "  'value': \"This is one of the most difficult challenges in our insurance operations, and it requires what I call unusual managerial discipline. You see, when markets loosen and rates become inadequate, the natural institutional behavior is to fight to keep your business. Nobody likes to see the other fellow take away their customers. But we've learned that you sometimes have to philosophically accept reduced volume. Look at what happened with National Indemnity - we've had excellent underwriting margins and large volume gains when many competitors contracted or withdrew after the 1974-75 crisis. But these conditions won't last forever. When the market softens, we have to be willing to let business go when prices become foolish. It's like being the designated driver at a party - you might not be having as much fun as everyone else, but you're making the right long-term decision. The key is maintaining that discipline even when it runs counter to what everyone else is doing. It's not easy, but it's essential for long-term success in the insurance business.\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]['conversations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c988bc-b1c9-48e2-8c85-ea9af8603738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "dataset = standardize_sharegpt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e406281e-f40a-44b6-81ec-5fb44b67b7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'How do you approach the challenge of maintaining discipline when market conditions soften and competitors start offering unrealistic rates?',\n",
       "  'role': 'user'},\n",
       " {'content': \"This is one of the most difficult challenges in our insurance operations, and it requires what I call unusual managerial discipline. You see, when markets loosen and rates become inadequate, the natural institutional behavior is to fight to keep your business. Nobody likes to see the other fellow take away their customers. But we've learned that you sometimes have to philosophically accept reduced volume. Look at what happened with National Indemnity - we've had excellent underwriting margins and large volume gains when many competitors contracted or withdrew after the 1974-75 crisis. But these conditions won't last forever. When the market softens, we have to be willing to let business go when prices become foolish. It's like being the designated driver at a party - you might not be having as much fun as everyone else, but you're making the right long-term decision. The key is maintaining that discipline even when it runs counter to what everyone else is doing. It's not easy, but it's essential for long-term success in the insurance business.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]['conversations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f69203b-fdab-4ebc-bee6-add74b9a0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d385d1-0315-46ac-8985-8731fbbe0681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'How do you approach the challenge of maintaining discipline when market conditions soften and competitors start offering unrealistic rates?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"This is one of the most difficult challenges in our insurance operations, and it requires what I call unusual managerial discipline. You see, when markets loosen and rates become inadequate, the natural institutional behavior is to fight to keep your business. Nobody likes to see the other fellow take away their customers. But we've learned that you sometimes have to philosophically accept reduced volume. Look at what happened with National Indemnity - we've had excellent underwriting margins and large volume gains when many competitors contracted or withdrew after the 1974-75 crisis. But these conditions won't last forever. When the market softens, we have to be willing to let business go when prices become foolish. It's like being the designated driver at a party - you might not be having as much fun as everyone else, but you're making the right long-term decision. The key is maintaining that discipline even when it runs counter to what everyone else is doing. It's not easy, but it's essential for long-term success in the insurance business.\",\n",
       "   'role': 'assistant'}],\n",
       " 'text': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do you approach the challenge of maintaining discipline when market conditions soften and competitors start offering unrealistic rates?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThis is one of the most difficult challenges in our insurance operations, and it requires what I call unusual managerial discipline. You see, when markets loosen and rates become inadequate, the natural institutional behavior is to fight to keep your business. Nobody likes to see the other fellow take away their customers. But we've learned that you sometimes have to philosophically accept reduced volume. Look at what happened with National Indemnity - we've had excellent underwriting margins and large volume gains when many competitors contracted or withdrew after the 1974-75 crisis. But these conditions won't last forever. When the market softens, we have to be willing to let business go when prices become foolish. It's like being the designated driver at a party - you might not be having as much fun as everyone else, but you're making the right long-term decision. The key is maintaining that discipline even when it runs counter to what everyone else is doing. It's not easy, but it's essential for long-term success in the insurance business.<|eot_id|>\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff6f943b-da37-46ab-aecd-a95c77a40621",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "# STEP 1: Pre-tokenize the dataset with add_special_tokens=False\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        add_special_tokens=False,  # ‚Üê This prevents the duplicate!\n",
    "        padding=False,  # Let DataCollator handle padding\n",
    "    )\n",
    "\n",
    "# Tokenize before splitting\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset.column_names  # Remove old columns\n",
    ")\n",
    "\n",
    "# STEP 2: Now split\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "\n",
    "# STEP 3: Apply train_on_responses_only AFTER tokenization\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# STEP 4: Create trainer WITHOUT dataset_text_field\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_split[\"train\"],\n",
    "    eval_dataset=dataset_split[\"test\"],\n",
    "    # Remove these two lines:\n",
    "    # dataset_text_field=\"text\",  ‚Üê DELETE THIS\n",
    "    # dataset_num_proc=4,  ‚Üê DELETE THIS (already tokenized)\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        # ... rest of your args stay the same\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.05,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=1e-4,\n",
    "        max_grad_norm=1.0,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/llama-buffett-2epochs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "# STEP 5: Apply response masking\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e08425e-756a-4dfc-944f-c6344e8d60b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf79f5ba54a44e19ecfe589c7205eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=56):   0%|          | 0/12515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddcdf6d605c45359e4ef3cb104c9b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=56):   0%|          | 0/2209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#EARLY STOPPING / TEST SIZE / EPOCHS / LEARNING RATE\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Fix: Handle if dataset is already a DatasetDict\n",
    "if hasattr(dataset, 'keys'):  # It's a DatasetDict\n",
    "    dataset = dataset[\"train\"]  # Get the train split\n",
    "\n",
    "# Now split it\n",
    "dataset_split = dataset.train_test_split(test_size=0.15, seed=42)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_split[\"train\"],  # 85% for training\n",
    "    eval_dataset = dataset_split[\"test\"],    # 15% for validation\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.05,\n",
    "        num_train_epochs = 2,\n",
    "        learning_rate = 1e-4,\n",
    "        max_grad_norm = 1.0,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 50,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 50,\n",
    "        save_total_limit = 3,\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs/llama-buffett-2epochs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2b425dc-14e1-4eca-aa3e-ace94f4f7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "225dbe42-bc2f-4281-889c-1806277a8d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a71bdd2ef445e6817089fbf240a97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=56):   0%|          | 0/12515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca0a8d037c146cda9c55a4a1f89e267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=56):   0%|          | 0/2209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce3e20b-7b1f-4a20-8d04-4c3ccf02b772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow does your long-term investment horizon influence your approach to keeping cash reserves?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe key thing to understand is that we have this really pretty unusual flexibility at Berkshire, combined with what I'd emphasize is a very long period ahead of us to find opportunities. We're not thinking quarter to quarter like many others might. We're measuring our cash position in terms of its potential utility over an extended period where we'll have opportunities to deploy funds. This long-term perspective means we can be patient and wait for those moments when we can deploy money at rates that may be quite a bit higher than other people might achieve. It's not just about having cash - it's about having the time horizon and patience to use it optimally. As Charlie points out, we're perfectly willing to pay what amounts to an insurance premium now - keeping substantial cash on hand - to ensure we have a lot of money available when something really attractive comes up in difficult times. This long-term approach has served us exceptionally well, as demonstrated during the financial crisis of 2008 and 2009, when our patience and preparedness really paid off.<|eot_id|>\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85591252-1687-425c-8408-b946219c6de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                   The key thing to understand is that we have this really pretty unusual flexibility at Berkshire, combined with what I'd emphasize is a very long period ahead of us to find opportunities. We're not thinking quarter to quarter like many others might. We're measuring our cash position in terms of its potential utility over an extended period where we'll have opportunities to deploy funds. This long-term perspective means we can be patient and wait for those moments when we can deploy money at rates that may be quite a bit higher than other people might achieve. It's not just about having cash - it's about having the time horizon and patience to use it optimally. As Charlie points out, we're perfectly willing to pay what amounts to an insurance premium now - keeping substantial cash on hand - to ensure we have a lot of money available when something really attractive comes up in difficult times. This long-term approach has served us exceptionally well, as demonstrated during the financial crisis of 2008 and 2009, when our patience and preparedness really paid off.<|eot_id|>\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a sample from the trainer's dataset\n",
    "sample = trainer.train_dataset[5]\n",
    "labels = sample[\"labels\"]\n",
    "\n",
    "# Now check what's being trained on\n",
    "space = tokenizer(text=\" \", add_special_tokens=False).input_ids[0]\n",
    "filtered = [space if x == -100 else x for x in labels]\n",
    "tokenizer.decode(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "502406f6-42f8-4615-ab89-52398fd6da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA H100 80GB HBM3. Max memory = 79.189 GB.\n",
      "76.65 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5067442-8816-41ec-9dfa-ceafb4adac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n",
      "   \\\\   /|    Num examples = 12,515 | Num Epochs = 2 | Total steps = 6,258\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 414,187,520 of 70,967,894,016 (0.58% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='6258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  27/6258 06:19 < 26:15:01, 0.07 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee6d25c3-cf79-4b0d-9296-ec000507bdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Q: What is your investment philosophy?\n",
      "==================================================\n",
      "A: Our investment philosophy is really quite simple - we look for good businesses at fair prices and then hold them for the long term. We believe in what I call "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     31\u001b[39m inputs = tokenizer(\n\u001b[32m     32\u001b[39m     prompt,\n\u001b[32m     33\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     35\u001b[39m ).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Generate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_streamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.92\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/site-packages/peft/peft_model.py:1973\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1971\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1972\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1973\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1975\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/site-packages/unsloth/models/llama.py:2032\u001b[39m, in \u001b[36munsloth_fast_generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2027\u001b[39m \u001b[38;5;66;03m# Mixed precision autocast\u001b[39;00m\n\u001b[32m   2028\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   2029\u001b[39m     torch.inference_mode(),\n\u001b[32m   2030\u001b[39m     torch.autocast(device_type = DEVICE_TYPE_TORCH, dtype = dtype),\n\u001b[32m   2031\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2032\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[38;5;66;03m# Return accelerate back\u001b[39;00m\n\u001b[32m   2035\u001b[39m \u001b[38;5;66;03m# if accelerate_new_send_to_device is not None:\u001b[39;00m\n\u001b[32m   2036\u001b[39m \u001b[38;5;66;03m#     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device\u001b[39;00m\n\u001b[32m   2037\u001b[39m \u001b[38;5;66;03m# pass\u001b[39;00m\n\u001b[32m   2039\u001b[39m FastLlamaModel.for_training(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/site-packages/transformers/generation/utils.py:2840\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2838\u001b[39m input_ids = torch.cat([input_ids, next_tokens[:, \u001b[38;5;28;01mNone\u001b[39;00m]], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m   2839\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streamer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2840\u001b[39m     \u001b[43mstreamer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_tokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2842\u001b[39m unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n\u001b[32m   2843\u001b[39m this_peer_finished = unfinished_sequences.max() == \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/site-packages/transformers/generation/streamers.py:117\u001b[39m, in \u001b[36mTextStreamer.put\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    114\u001b[39m     printable_text = text[\u001b[38;5;28mself\u001b[39m.print_len : text.rfind(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m) + \u001b[32m1\u001b[39m]\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mself\u001b[39m.print_len += \u001b[38;5;28mlen\u001b[39m(printable_text)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_finalized_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprintable_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/site-packages/transformers/generation/streamers.py:135\u001b[39m, in \u001b[36mTextStreamer.on_finalized_text\u001b[39m\u001b[34m(self, text, stream_end)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_finalized_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, stream_end: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    134\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the new text to stdout. If the stream is ending, also prints a newline.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28mprint\u001b[39m(text, flush=\u001b[38;5;28;01mTrue\u001b[39;00m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream_end \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/site-packages/ipykernel/iostream.py:604\u001b[39m, in \u001b[36mOutStream.flush\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28mself\u001b[39m.pub_thread.schedule(evt.set)\n\u001b[32m    603\u001b[39m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    605\u001b[39m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[32m    606\u001b[39m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[32m    607\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIOStream.flush timed out\u001b[39m\u001b[33m\"\u001b[39m, file=sys.__stderr__)\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/unsloth/lib/python3.11/threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Set model to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Enable streaming\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is your investment philosophy?\",\n",
    "    \"How do you evaluate a company?\",\n",
    "    \"What makes a good business?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(\"A: \", end=\"\")\n",
    "    \n",
    "    # Use the tokenizer's apply_chat_template\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.75,\n",
    "        top_p=0.92,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73fc0c85-fe74-415c-8306-692295fafaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MULTI-TURN CONVERSATION TEST\n",
      "============================================================\n",
      "\n",
      "[Previous conversation about investment philosophy and See's Candy...]\n",
      "\n",
      "User: You mentioned See's has given you over $1 billion pretax on a $25 million investment. What specifically about their pricing power made those economics so attractive? And how much of that $40 million you put in was actually needed versus optional expansion?\n",
      "Warren: Well, let me tell you about that fascinating story. See's was already doing $38 million in sales when we bought them. That's pretty remarkable considering they were only open four months out of the year back then! Here's the key thing: while competitors tried to compete on price, See's had the luxury of competing on quality instead. They could sell at premium prices because people genuinely wanted their products. Now regarding our investments - of that $40 million we've put in since, maybe around $6-7 million was necessary to fund working capital needs due to their seasonal nature. But most of it went toward opening new stores, which was purely an option. We weren't forced to do any of it - but we did it anyway because we saw clear opportunities for growth within our circle of competence.<|im_end|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set model to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Enable streaming\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Simulate a multi-turn conversation where follow-up relies on previous context\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your investment philosophy?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Our approach is really quite simple - we're looking for large businesses with understandable, enduring and what I like to call 'mouth-watering' economics. But it's not just about the business itself. We want these companies to be run by able and shareholder-oriented managements. And here's something crucial - we need a sensible price tag attached. We stick to what we understand and what has worked for us over time.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you give me a specific example of a company that demonstrates this philosophy?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Look at See's Candy. When we bought it in 1972 for $25 million, it had exactly those characteristics I just mentioned - understandable business, mouth-watering economics, and great management. The brand had real pricing power, and customers loved it. Since then, we've put $40 million into it and it's given us over $1 billion pretax.\"},\n",
    "    {\"role\": \"user\", \"content\": \"You mentioned See's has given you over $1 billion pretax on a $25 million investment. What specifically about their pricing power made those economics so attractive? And how much of that $40 million you put in was actually needed versus optional expansion?\"}\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MULTI-TURN CONVERSATION TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n[Previous conversation about investment philosophy and See's Candy...]\")\n",
    "print(f\"\\nUser: {conversation[-1]['content']}\")\n",
    "print(\"Warren: \", end=\"\")\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.75,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c957392-13f1-4446-b870-337bdc5b9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.quantization import quantize_\n",
    "from torchao.quantization.qat import QATConfig\n",
    "quantize_(model, QATConfig(step = \"convert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00d70f57-f74b-4956-bcd3-709f7d66fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.quantization import Int4WeightOnlyConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06dae02d-fe35-4b1b-b74c-1a3c37492fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_key = \"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\"\n",
    "repo = \"BuffettBot-QwenQAT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b8dbe5-d01e-4247-bdd6-8702e9711307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/brokenlander/BuffettBot-QwenQAT', endpoint='https://huggingface.co', repo_type='model', repo_id='brokenlander/BuffettBot-QwenQAT')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import delete_repo, create_repo\n",
    "\n",
    "delete_repo(\n",
    "    repo_id=repo,\n",
    "    token=hf_key,\n",
    "    missing_ok = True\n",
    ")\n",
    "\n",
    "# Create the repo\n",
    "create_repo(\n",
    "    repo_id=repo,\n",
    "    token=hf_key,\n",
    "    private=True,\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cacba24-fa14-47aa-a4d4-6da3f8f0c4db",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsloth_save_pretrained_torchao() got an unexpected keyword argument 'commit_message'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained_torchao\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbrokenlander/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorchao_config\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mInt4WeightOnlyConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInitial Commit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQAT fine-tuned model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsloth_save_pretrained_torchao() got an unexpected keyword argument 'commit_message'"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_torchao(\n",
    "    \"brokenlander/\" + repo,\n",
    "    tokenizer,\n",
    "    torchao_config = Int4WeightOnlyConfig(),\n",
    "    push_to_hub = True,\n",
    "    token = hf_key,\n",
    "    commit_message = \"Initial Commit\",\n",
    "    commit_description = \"QAT fine-tuned model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c587e9-ea19-451b-9814-f0843e460d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb22907-9e15-4905-a691-94f4e11a3f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199361d-ea4b-46a4-856c-1dc1520aa5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unsloth)",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
