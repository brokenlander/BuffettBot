{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272f88a8-c392-44ed-af7b-38a27aeb901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import spacy\n",
    "import re\n",
    "import fitz\n",
    "from openai import OpenAI, Client\n",
    "from anthropic import Anthropic\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73cc985-0cef-4594-862f-5d4fef7505eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Anthropic(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74b00ee-f8ac-4ac6-b013-840fa78c68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sentence:\n",
    "    \"\"\"Simple container for sentence text and length.\"\"\"\n",
    "    text: str\n",
    "    length: int\n",
    "\n",
    "class SplittingStrategy(Enum):\n",
    "    NUMBERED_SECTIONS = \"numbered_sections\"\n",
    "    SENTENCE_OVERLAP = \"sentence_overlap\"\n",
    "\n",
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    \"\"\"Configuration for text chunking and processing.\"\"\"\n",
    "    max_chunk_size: int = 3000\n",
    "    min_chunk_size: int = 800\n",
    "    overlap_sentences: int = 2\n",
    "    strategy: SplittingStrategy = SplittingStrategy.NUMBERED_SECTIONS\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, client: Anthropic, config: ChunkConfig):\n",
    "        \"\"\"Initialize the document processor.\"\"\"\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['tagger', 'ner'])\n",
    "        self.nlp.max_length = 10000000\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text while preserving important newlines.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        lines = [' '.join(line.split()) for line in lines if line.strip()]\n",
    "        return '\\n'.join(lines)\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[str]:\n",
    "        \"\"\"Extract text from PDF and split into chunks.\"\"\"\n",
    "        text = \"\"\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text() + \"\\n\"\n",
    "        \n",
    "        clean_text = self.preprocess_text(text)\n",
    "        return self._chunk_text(clean_text)\n",
    "\n",
    "    def process_txt(self, txt_path: str) -> List[str]:\n",
    "        \"\"\"Read text file and split into chunks.\"\"\"\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            clean_text = self.preprocess_text(text)\n",
    "            return self._chunk_text(clean_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text file {txt_path}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _split_by_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text using sentence overlap strategy.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [Sentence(text=sent.text.strip(), length=len(sent.text.strip())) \n",
    "                    for sent in doc.sents if sent.text.strip()]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence.length\n",
    "            \n",
    "            if current_length >= self.config.max_chunk_size and len(current_chunk) > self.config.overlap_sentences:\n",
    "                if current_length >= self.config.min_chunk_size:\n",
    "                    chunks.append(' '.join(s.text for s in current_chunk))\n",
    "                    overlap_sentences = current_chunk[-self.config.overlap_sentences:]\n",
    "                    current_chunk = overlap_sentences.copy()\n",
    "                    current_length = sum(s.length for s in current_chunk)\n",
    "\n",
    "        if current_length >= self.config.min_chunk_size:\n",
    "            chunks.append(' '.join(s.text for s in current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def _split_by_numbered_sections(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text based on numbered sections.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        initial_chunks = []\n",
    "        current_chunk = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if re.match(r'^\\d+\\.', line.strip()):\n",
    "                if current_chunk:\n",
    "                    initial_chunks.append('\\n'.join(current_chunk))\n",
    "                current_chunk = [line]\n",
    "            else:\n",
    "                current_chunk.append(line)\n",
    "        \n",
    "        if current_chunk:\n",
    "            initial_chunks.append('\\n'.join(current_chunk))\n",
    "        \n",
    "        # Handle chunks that are too large\n",
    "        final_chunks = []\n",
    "        for chunk in initial_chunks:\n",
    "            if len(chunk) > self.config.max_chunk_size:\n",
    "                doc = self.nlp(chunk)\n",
    "                sentences = list(doc.sents)\n",
    "                mid_point = len(sentences) // 2\n",
    "                first_half = ' '.join(sent.text.strip() for sent in sentences[:mid_point])\n",
    "                second_half = ' '.join(sent.text.strip() for sent in sentences[mid_point:])\n",
    "                final_chunks.extend([first_half, second_half])\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "                \n",
    "        return final_chunks\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text using the configured strategy.\"\"\"\n",
    "        if self.config.strategy == SplittingStrategy.NUMBERED_SECTIONS:\n",
    "            return self._split_by_numbered_sections(text)\n",
    "        else:\n",
    "            return self._split_by_sentences(text)\n",
    "\n",
    "    def _parse_qa_format(self, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Parse the QUESTION_N / ANSWER_N format into structured data.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Find all questions and answers\n",
    "        questions = re.findall(r'QUESTION_\\d+:\\s*(.*?)(?=ANSWER_|\\Z)', text, re.DOTALL)\n",
    "        answers = re.findall(r'ANSWER_\\d+:\\s*(.*?)(?=QUESTION_|\\Z)', text, re.DOTALL)\n",
    "        \n",
    "        if len(questions) != len(answers):\n",
    "            raise ValueError(f\"Mismatched Q&A count: {len(questions)} questions, {len(answers)} answers\")\n",
    "        \n",
    "        qa_pairs = []\n",
    "        for q, a in zip(questions, answers):\n",
    "            qa_pairs.append({\n",
    "                \"question\": q.strip(),\n",
    "                \"answer\": a.strip()\n",
    "            })\n",
    "        \n",
    "        return qa_pairs\n",
    "\n",
    "    def generate_conversation(self, chunk: str) -> Optional[Dict]:\n",
    "        \"\"\"Generate conversation from chunk with validation.\"\"\"\n",
    "        try:\n",
    "            validation_prompt = f\"\"\"Analyze this text and determine if it contains meaningful Warren Buffett insights, commentary, or narrative content.\n",
    "\n",
    "Approve the text only if:\n",
    "- It discusses business philosophy or investment thinking that applies across industries and time.\n",
    "- It provides views on markets, financial practices, or economic principles that are broadly applicable.\n",
    "- Buffett shares personal reflections or general lessons learned that are useful beyond a single event.\n",
    "\n",
    "Reject the text if:\n",
    "- It primarily describes a specific investment, acquisition, deal, or financial transaction.\n",
    "- It focuses on a single company's business decision without a clearly stated general principle.\n",
    "- It discusses short-term market conditions, quarterly earnings, or economic events without broader insights.\n",
    "- It contains only financial data, figures, or statistics without meaningful explanation.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Return ONLY \"yes\" if the text contains meaningful, wide-scope content, or \"no\" otherwise. I WANT A SINGLE YES or NO!!\"\"\"\n",
    "            \n",
    "            validation_response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-5-20250929\",\n",
    "                messages=[{\"role\": \"user\", \"content\": validation_prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            if validation_response.content[0].text.strip().lower() != \"yes\":\n",
    "                return None\n",
    "\n",
    "            conversation_prompt = f\"\"\"Below is a text excerpt from me (Warren Buffett). Your task is to generate 2 substantive questions about the key themes in this content, followed by detailed answers in my characteristic Q&A style from the annual meetings.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "CRITICAL LENGTH REQUIREMENT:\n",
    "- Each answer MUST be 400-600 words minimum\n",
    "- Answers under 400 words are TOO SHORT and inadequate\n",
    "- Think of this as a teaching opportunity, not a quick soundbite\n",
    "\n",
    "Question Guidelines:\n",
    "- Focus on the main principles, ideas, or philosophical points in the text\n",
    "- Ask questions that invite thorough, multi-faceted explanations\n",
    "- Avoid yes/no questions or those requiring only brief factual answers\n",
    "\n",
    "Answer Structure & Style:\n",
    "1. OPENING: Start with a direct, clear answer to the question (2-3 sentences)\n",
    "\n",
    "2. REASONING: Walk through the logic step-by-step\n",
    "   - Explain WHY things work this way\n",
    "   - Break down the underlying principles\n",
    "   - Show the cause-and-effect relationships\n",
    "\n",
    "3. EXAMPLES & ANALOGIES: Use concrete illustrations from the source text\n",
    "   - If the text contains analogies or metaphors, develop them fully\n",
    "   - If the text references specific examples or comparisons, explain why they matter\n",
    "   - When the text discusses concrete situations, use them to illuminate principles\n",
    "   - Stay grounded in what is actually present in the source material\n",
    "\n",
    "4. BROADER IMPLICATIONS: Connect to bigger themes\n",
    "   - How does this principle apply more widely?\n",
    "   - What are the long-term consequences?\n",
    "   - What lessons can investors and managers take away?\n",
    "\n",
    "5. PRACTICAL APPLICATION: Ground it in reality\n",
    "   - How does this play out in actual business situations?\n",
    "   - What should thoughtful people do with this information?\n",
    "   - What pitfalls should they avoid?\n",
    "\n",
    "Voice & Tone:\n",
    "- Write in first person as Warren Buffett\n",
    "- Maintain my conversational, accessible speaking style\n",
    "- Be patient and thorough in explanations, like teaching at the annual meeting\n",
    "- Use simple language to explain complex ideas\n",
    "- Show enthusiasm for business principles and clear thinking\n",
    "- Be direct and honest, avoiding corporate-speak or jargon\n",
    "- Use natural language including contractions, possessives, and quotations as needed\n",
    "\n",
    "REMEMBER: Each answer should naturally span 400-600 words through thorough development of ideas. Develop the reasoning, explore the implications, and teach the concepts fully.\n",
    "\n",
    "FORMAT YOUR RESPONSE EXACTLY LIKE THIS:\n",
    "\n",
    "QUESTION_1: [Your first question here]\n",
    "\n",
    "ANSWER_1: [Your first detailed answer here - 400-600 words]\n",
    "\n",
    "QUESTION_2: [Your second question here]\n",
    "\n",
    "ANSWER_2: [Your second detailed answer here - 400-600 words]\n",
    "\n",
    "Do not include any other text, formatting, or explanations. Just the four sections above.\"\"\"\n",
    "\n",
    "            conversation_response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-5-20250929\",\n",
    "                messages=[{\"role\": \"user\", \"content\": conversation_prompt}],\n",
    "                max_tokens=5000,\n",
    "                temperature=0.4\n",
    "            )\n",
    "            \n",
    "            response_text = conversation_response.content[0].text.strip()\n",
    "            \n",
    "            # Parse the simple format\n",
    "            try:\n",
    "                qa_pairs = self._parse_qa_format(response_text)\n",
    "                if qa_pairs and len(qa_pairs) == 2:\n",
    "                    # Build the ShareGPT JSON structure in Python (with proper escaping)\n",
    "                    conversation_data = {\n",
    "                        \"conversations\": [\n",
    "                            [\n",
    "                                {\"from\": \"human\", \"value\": qa_pairs[0][\"question\"]},\n",
    "                                {\"from\": \"gpt\", \"value\": qa_pairs[0][\"answer\"]}\n",
    "                            ],\n",
    "                            [\n",
    "                                {\"from\": \"human\", \"value\": qa_pairs[1][\"question\"]},\n",
    "                                {\"from\": \"gpt\", \"value\": qa_pairs[1][\"answer\"]}\n",
    "                            ]\n",
    "                        ]\n",
    "                    }\n",
    "                    return conversation_data\n",
    "                else:\n",
    "                    print(f\"Failed to parse exactly 2 Q&A pairs\")\n",
    "                    return None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing Q&A format: {str(e)}\")\n",
    "                print(f\"Response text: {response_text[:200]}...\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {chunk[:100]}...\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b9c794-61d1-4575-838f-e224aa707cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ChunkConfig(\n",
    "    max_chunk_size=3000,\n",
    "    min_chunk_size=800,\n",
    "    overlap_sentences=2, \n",
    "    strategy=SplittingStrategy.SENTENCE_OVERLAP  # Or NUMBERED_SECTIONS if source has clear sections\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d51065e-6c80-4f61-a47b-aaded0133f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 189 chunks from the PDF\n"
     ]
    }
   ],
   "source": [
    "# Test with a single PDF file\n",
    "processor = DocumentProcessor(client, config)\n",
    "test_pdf_path = \"Dataset/Unprocessed/Lessons for Corporate America/Lessons-for-Corporate-America.pdf\"\n",
    "chunks = processor.process_pdf(test_pdf_path)\n",
    "print(f\"Generated {len(chunks)} chunks from the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d76c7c-1ffa-4eea-a5f4-1475722a700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample conversation:\n",
      "{\n",
      "  \"conversations\": [\n",
      "    [\n",
      "      {\n",
      "        \"from\": \"human\",\n",
      "        \"value\": \"You compare seeking different types of shareholders to a restaurant that can't decide between French cuisine and take-out chicken. Why is shareholder stability so important to you, and what's wrong with managements that encourage high trading volume in their stock?\"\n",
      "      },\n",
      "      {\n",
      "        \"from\": \"gpt\",\n",
      "        \"value\": \"The answer is straightforward: you simply cannot run a good business if you're constantly trying to please people who want fundamentally different things. When management encourages high trading volume, they're essentially saying they want a revolving door of owners, and that makes absolutely no sense to me.\\n\\nLet me walk you through the logic here. Every type of shareholder comes with different expectations. Some want high current dividends. Others want rapid capital appreciation. Still others are just looking for short-term trading profits. Now, if you try to satisfy all these groups simultaneously, you end up satisfying none of them. It's exactly like that restaurant analogy. Imagine you walk into a place expecting coq au vin and fine wine, but half the menu is fried chicken in a bucket. You'd be confused and disappointed. The customers who wanted take-out chicken would be equally unhappy with the white tablecloths and high prices. Pretty soon, nobody's happy and everyone leaves.\\n\\nThe same principle applies to corporations. When you seek high trading activity, you're actively courting new shareholders with different expectations while pushing out the existing ones. You can't add lots of new owners without losing lots of former owners\\u2014that's just mathematics. And here's what puzzles me: why would any management want that? Why would you want to lose shareholders who understand your business, approve of your policies, and share your long-term expectations? It makes no sense.\\n\\nAt Berkshire, Charlie and I have worked hard to attract a specific type of owner. We want people who think like partners, who understand that we're focused on long-term business value rather than short-term stock price movements. We want people who are comfortable with our approach of retaining earnings when we can deploy them effectively, rather than demanding high dividends. We want people who will stick with us through the inevitable ups and downs of business.\\n\\nAnd here's the beautiful thing: when you're clear about what you are and consistent in your approach, you naturally attract the right constituency. Our shareholders return year after year, just like customers at a good restaurant. They know what they're getting. They like the service and the menu. We have very low turnover, and I consider the people sitting in Berkshire's shareholder \\\"seats\\\" to be just about the best group we could possibly hope for.\\n\\nThis stability creates a virtuous cycle. Because our shareholders understand our operation and share our time horizon, we don't have to waste time and energy managing short-term stock price expectations. We can focus entirely on building long-term business value. We don't have to explain ourselves constantly to new faces or adjust our strategy to accommodate flavor-of-the-month demands. Instead, we can simply do what we said we'd do and deliver on the expectations we've set.\\n\\nThe managements that seek high trading volume are essentially choosing chaos over stability, confusion over clarity. They're making their own jobs harder while diminishing the experience for everyone involved. We've chosen the opposite path, and it's worked out pretty well for us.\"\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"from\": \"human\",\n",
      "        \"value\": \"You describe Berkshire's annual meetings as productive forums for business discussion, unlike many corporate annual meetings that devolve into theatrics and ego-driven commentary. What makes Berkshire's meetings different, and how does this reflect your broader philosophy about the relationship between management and shareholders?\"\n",
      "      },\n",
      "      {\n",
      "        \"from\": \"gpt\",\n",
      "        \"value\": \"Berkshire's annual meetings work because we've attracted the right kind of shareholders and because Charlie and I treat the meeting as a genuine business forum rather than a corporate ritual to be endured. The difference comes down to mutual respect and aligned interests between management and owners.\\n\\nLet me explain what goes wrong at most annual meetings. Too often, they become completely unproductive for a couple of reasons. Sometimes management is the problem\\u2014they're reluctant to open up on matters of real business substance, so the whole thing becomes a scripted performance with no real content. But more often, the problem is with certain shareholder participants who are more interested in their moment on stage than in the actual affairs of the corporation. Think about the perverse incentive structure: for the price of one share, you get a captive audience for your views on how the world should be run. It's irresistible to some people.\\n\\nWhen this happens, the meeting deteriorates into theatrics, spleen-venting, and advocacy of pet issues that have nothing to do with the business. And here's the tragedy: this drives away the very people who should be attending. Shareholders who are genuinely interested in the business stop coming because they don't want to sit through hours of nonsense. The quality degrades year after year as the serious people are replaced by the theatrical ones.\\n\\nBerkshire's meetings are the complete opposite. Our attendance grows each year, and we haven't experienced silly questions or ego-inspired commentary. Instead, we get thoughtful questions about the business from people who actually care about understanding it better. This didn't happen by accident\\u2014it's a direct result of the shareholder constituency we've cultivated over the years.\\n\\nBecause we've been clear and consistent about our business principles and approach, we've attracted owners who think like partners. These aren't people looking to grandstand or push unrelated agendas. They're business-minded individuals who want to understand how their company operates and how management thinks about various challenges and opportunities. That's exactly the kind of dialogue Charlie and I enjoy, so we're happy to answer questions for as long as it takes.\\n\\nThis reflects our broader philosophy that we're all partners in this enterprise. Charlie and I think of ourselves as managing partners and our shareholders as owner-partners. In a real partnership, you don't play games or hide behind corporate formality. You have honest, substantive discussions about the business. You share information openly, with the only exception being matters where candor might cost the company real money\\u2014like our specific activities in securities.\\n\\nThe annual meeting is the proper time and place for these discussions. We can't respond to individual written or phoned questions throughout the year because one-person-at-a-time reporting is a poor use of management time when you have thousands of shareholders. But at the annual meeting, everyone hears the same information at the same time, and we can address questions that are likely of interest to many owners.\\n\\nThis approach creates a reinforcing cycle. Good meetings attract serious shareholders. Serious shareholders ask good questions. Good questions lead to substantive discussions. Substantive discussions make the meetings valuable. Valuable meetings attract more serious shareholders. And so on. It's the exact opposite of the death spiral you see at companies where meetings become theatrical wastes of time.\\n\\nThe key insight is that corporate governance isn't just about formal structures and rules. It's about fostering the right relationship between management and owners\\u2014a relationship built on mutual respect, shared expectations, and genuine partnership.\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    conversation = processor.generate_conversation(chunks[23])\n",
    "    print(\"Sample conversation:\")\n",
    "    print(json.dumps(conversation, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6962d3-247a-4b42-b5f7-05407c771ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_dir: str, output_dir: str, client: Anthropic, config: ChunkConfig):\n",
    "    \"\"\"Process all files in a directory and save conversations to JSON files.\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = DocumentProcessor(client, config)\n",
    "    \n",
    "    # Get all files in input directory\n",
    "    input_path = Path(input_dir)\n",
    "    files = sorted(input_path.glob('*'))\n",
    "    \n",
    "    for file_path in files:\n",
    "        # Skip directories\n",
    "        if file_path.is_dir():\n",
    "            continue\n",
    "            \n",
    "        # Only process .txt and .pdf files\n",
    "        if file_path.suffix.lower() not in ['.txt', '.pdf']:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Process the file based on its type\n",
    "            if file_path.suffix.lower() == '.pdf':\n",
    "                chunks = processor.process_pdf(str(file_path))\n",
    "            else:  # .txt\n",
    "                chunks = processor.process_txt(str(file_path))\n",
    "            \n",
    "            # Generate conversations for all chunks\n",
    "            all_conversations = []\n",
    "            for chunk in chunks:\n",
    "                conversation = processor.generate_conversation(chunk)\n",
    "                if conversation and 'conversations' in conversation:\n",
    "                    # Add all conversations from this chunk\n",
    "                    all_conversations.extend(conversation['conversations'])\n",
    "            \n",
    "            # Save to JSON file with same name but .json extension\n",
    "            output_filename = file_path.stem + '.json'\n",
    "            output_path = Path(output_dir) / output_filename\n",
    "            \n",
    "            # Write the combined conversations to file\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\"conversations\": all_conversations}, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"Successfully saved conversations to {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            # Save whatever we got so far\n",
    "            if 'all_conversations' in locals() and all_conversations:\n",
    "                output_filename = file_path.stem + '.json'\n",
    "                output_path = Path(output_dir) / output_filename\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump({\"conversations\": all_conversations}, f, indent=2, ensure_ascii=False)\n",
    "                print(f\"Successfully saved conversations to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4955239-6499-4159-8bda-2fc54b8f7945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2002pdf.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2002pdf.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2003ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2003ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2004ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2004ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2005ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2005ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2006ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2006ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2007ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2007ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2008ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2008ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2009ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2009ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2010ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2010ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2011ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2011ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2012ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2012ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2013ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2013ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2014ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2014ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2015ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2015ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2016ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2016ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2017ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2017ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2018ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2018ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2019ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2019ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2020ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2020ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2021ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2021ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2022ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2022ltr.json\n",
      "Processing Dataset\\Unprocessed\\Shareholder Letters\\2023ltr.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Shareholder Letters\\2023ltr.json\n"
     ]
    }
   ],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Shareholder Letters/\"\n",
    "output_directory = \"Dataset/Processed/Shareholder Letters/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0d62b1-b3a7-4d52-83e0-7fa4f9fd02b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset\\Unprocessed\\Lessons for Corporate America\\Lessons-for-Corporate-America.pdf\n",
      "Successfully saved conversations to Dataset\\Processed\\Lessons for Corporate America\\Lessons-for-Corporate-America.json\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"Dataset/Unprocessed/Lessons for Corporate America/\"\n",
    "output_directory = \"Dataset/Processed/Lessons for Corporate America/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c781c7c-d0d0-4ede-bbce-8b8cd77a4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ChunkConfig(\n",
    "    max_chunk_size=2400,\n",
    "    strategy=SplittingStrategy.NUMBERED_SECTIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4fc3cd5-0268-4fbd-932e-d6ac4313265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3450 chunks from the PDF\n"
     ]
    }
   ],
   "source": [
    "# Test with a single PDF file\n",
    "processor = DocumentProcessor(client, config)\n",
    "test_pdf_path = \"Dataset/Unprocessed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.pdf\"\n",
    "chunks = processor.process_pdf(test_pdf_path)\n",
    "print(f\"Generated {len(chunks)} chunks from the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2710532d-1d4d-4f80-bd43-0cae69dd1724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset\\Unprocessed\\Meeting Transcripts\\Berkshire Meeting Transcripts - 1994 - 2022.pdf\n",
      "Error processing chunk: 20. Weapons of mass destruction pose biggest risk to Berkshire\n",
      "WARREN BUFFETT: OK. Becky.\n",
      "BECKY QUIC...\n",
      "Error details: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}, 'request_id': 'req_011CV8xkaskZUhsYWpPkjhqJ'}\n",
      "Successfully saved conversations to Dataset\\Processed\\Meeting Transcripts\\Berkshire Meeting Transcripts - 1994 - 2022.json\n"
     ]
    }
   ],
   "source": [
    "# Process entire directory\n",
    "input_directory = \"Dataset/Unprocessed/Meeting Transcripts/\"\n",
    "output_directory = \"Dataset/Processed/Meeting Transcripts/\"\n",
    "process_directory(input_directory, output_directory, client, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eb19d26-0553-4f77-a622-5b105af971fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(input_directory, output_path):\n",
    "    \"\"\"\n",
    "    Merge all JSON files in the specified directory into a single JSON file\n",
    "    maintaining the nested 'conversations' structure.\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing JSON files\n",
    "        output_path (str): Full path (including filename) for the output merged JSON file\n",
    "    \"\"\"\n",
    "    # Initialize the merged structure\n",
    "    merged_data = {\n",
    "        \"conversations\": []\n",
    "    }\n",
    "    \n",
    "    # Convert string paths to Path objects\n",
    "    directory = Path(input_directory)\n",
    "    output = Path(output_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all JSON files in the directory\n",
    "    for file_path in directory.glob(\"*.json\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Check if the file has the expected structure\n",
    "                if \"conversations\" in data:\n",
    "                    # Extend the conversations list with the new data\n",
    "                    merged_data[\"conversations\"].extend(data[\"conversations\"])\n",
    "                else:\n",
    "                    print(f\"Warning: File {file_path} does not have the expected structure\")\n",
    "                    \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not parse JSON from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Write the merged data to the specified output path\n",
    "    try:\n",
    "        with open(output, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Successfully created merged file at: {output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing merged file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab7d5aa7-9e56-4010-9536-060b09498b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created merged file at: Dataset\\Processed\\Shareholder Letters\\Letters.json\n"
     ]
    }
   ],
   "source": [
    "merge_json_files('Dataset/Processed/Shareholder Letters/', 'Dataset/Processed/Shareholder Letters/Letters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c26a861-4cf8-4be9-aca1-5bf745ed857c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dataset/Processed/Ground Truth/Transcripts.json'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs('Dataset/Processed/Ground Truth', exist_ok=True)\n",
    "shutil.copy2('Dataset/Processed/Shareholder Letters/Letters.json', 'Dataset/Processed/Ground Truth/Letters.json')\n",
    "shutil.copy2('Dataset/Processed/Lessons for Corporate America/Lessons-for-Corporate-America.json', 'Dataset/Processed/Ground Truth/Lessons.json')\n",
    "shutil.copy2('Dataset/Processed/Meeting Transcripts/Berkshire Meeting Transcripts - 1994 - 2022.json', 'Dataset/Processed/Ground Truth/Transcripts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8939a8d4-71de-418a-8191-dc51f4978450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created merged file at: Dataset\\Processed\\Ground Truth\\dataset_combined.json\n"
     ]
    }
   ],
   "source": [
    "merge_json_files('Dataset/Processed/Ground Truth/', 'Dataset/Processed/Ground Truth/dataset_combined.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219be00-617e-403e-92a9-1c1904845509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
